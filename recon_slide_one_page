Title: Reconciliation Process Overview
Step-by-Step Breakdown:
Read batch information: Fetch BATCHID, BATCH_START_DATE, and BATCH_END_DATE from the Oracle batch table.
Query source Oracle database: Retrieve document count within the specified date range.
Query target MongoDB database: Retrieve document count within the specified date range.
Compare key columns: Compare DOMAIN, DIVISION, SUBDIVISION, CUSTREFKEY, CUSTREFValue, and EXTREFKEY, EXTREFValue between source and target data.
Document validation: Randomly sample documents and validate their existence in Hitachi Content Platform.
Update reconciliation status: Update SparkReconBatchTable with counts, comparison results, and validation results.

Understanding the Task
We aim to compare specific columns in two PySpark DataFrames: source_df and target_df. The goal is to identify discrepancies between these columns.

Explanation
Create DataFrames: We create sample DataFrames source_df and target_df with the specified columns.
Full Outer Join: We perform a full outer join on the comparison columns. This will include all records from both DataFrames, with null values for columns that don't match.
Identify Mismatches: We filter the joined DataFrame based on mismatched id columns. This indicates records that exist in one DataFrame but not the other, or where the values for the comparison columns differ.

comparison_df = source_df.join(target_df, ["DOMAIN", "DIVISION", "CUSTREFKEY", "CUSTREFValue", "EXTREFKEY", "EXTREFValue"], how="full_outer")

# Identify records that exist in one DataFrame but not the other
mismatch_count = comparison_df.filter((comparison_df["id"] != comparison_df["id"])).count()
print("Mismatch count:", mismatch_count)

# Further analysis can be done based on the comparison_df

We aim to randomly sample documents from a PySpark DataFrame, extract a relevant field (e.g., HCP storage path), and validate the existence of the corresponding document in the Hitachi Content Platform (HCP).

Explanation
Create Sample DataFrame: We create a sample DataFrame with columns id, document_name, and HCP_storage_path.
Random Sampling: We randomly sample 10% of the DataFrame using sample.
Extract HCP Paths: We extract the HCP_storage_path column into a Python list.
Validate Documents: Iterate over the HCP paths, calling validate_document for each path.
Return Results: Return a list of tuples containing the HCP path and validation status.


Understanding the Task
We aim to retrieve document counts within a specified date range from both an Oracle database and a MongoDB collection.

Explanation
Function Definition: The get_document_counts function takes the start and end dates as input and returns a tuple containing the source and target counts.
Oracle Query: Constructs a SQL query to count documents within the specified date range in the Oracle database. Uses pandas.read_sql to execute the query and extract the count.
MongoDB Query: Constructs a MongoDB aggregation pipeline to count documents within the specified date range. Uses PySpark's MongoDB connector to execute the query and extract the count.
Return Values: Returns the source_count and target_count as a tuple.



Key Steps:

Data Ingestion: Fetch batch information, extract data from Oracle and MongoDB.
Data Comparison: Compare key columns to identify discrepancies.
Document Validation: Randomly sample documents and validate on HCP.
Reconciliation Update: Update reconciliation status with metrics.
Key Metrics:

Total records processed
Match rate
Mismatch rate
Validation success rate
Average reconciliation time


Crafting a Strong Summary Statement
Summarize the overall goal: Reconcile data between Oracle and MongoDB, ensuring data consistency and integrity.
Highlight key achievements: Successfully implemented a process to compare key columns, validate data in HCP, and update reconciliation status efficiently.
Quantify impact: If possible, include metrics or results to demonstrate the value of the process.

"This reconciliation process effectively compares and validates millions of documents between Oracle and MongoDB, ensuring data consistency and integrity. By leveraging PySpark and HCP integration, we aim to optimize performance and accuracy in identifying discrepancies."



Relational TABLE
---------------------------------------------
P8GUID
DOMAIN
DIVISION
SUBDIVISION
SECURITY GROUP 
CONTENT_LAST_ACCESSED_DATE 
MIME_TYPE
HCP_BLOB_DATA
CHANNELCODE
EXT_REF_DOMAIN
EXT_REF_KEY_NAME
EXT_REF_KEY_VALUE
CUST_REF_DOMAIN
CUST_REF_KEY_NAME
CUST_REF_KEY_VALUE
TUPLEID
DOCUMENT_CAT_CODE
DOCUMENT_CAT_DESC
DOCUMENT_TYPE_CODE
DOCUMENT_TYPE_DESC
DOCUMENT_SUBTYPE_CODE
DOCUMENT_SUBTYPE_DESC
DOCUMENT_DATE_FORMAT
jurisdiction
dba_name
doc_day
doc_month
doc_year
documentStateList
documentstatus
contentsize
createdBy
systemCreatedDateTime
systemModifiedDateTime
modifiedBy
documentVersion
initialVersionid
documentversionLabel
versionTags
documentStatusDateTime
documentTitle
firewall_privacy_booking
firewall_privacy_jurisdi
gci_no
gisClass
documentCreatedDateTime
LABEL
grmHold
LEGAL_NAME
lockOwner
lockCreationDateTime
merchant_id
pageCount
originationSystemBatchUid
originationSystemCaptureTime
originationSystemDocId
originationSystemName
prev_doc_id
prev_doc_source
recordCode
recordTriggerStartDateTime
rtm_id
sent_by
sourceDocDescription
updated_by



JSON SCHEMA
____________________________________________
documentOS.documentOsGuid
bankDocument.tenants.DOMAIN
bankDocument.tenants.DIVISION
bankDocument.tenants.SUBDIVISION
bankDocument.tenants.SECURITY GROUP 
CONTENT_LAST_ACCESSED_DATE 
MIME_TYPE
storage.documentStoreUrl
adminDocument.ChannelCode
externalReferenceKeys.DOMAIN
externalReferenceKeys.NAME
externalReferenceKeys.VALUE
customerReferenceKeys.DOMAIN
customerReferenceKeys.NAME
customerReferenceKeys.VALUE
tuple.TUPLEID
tuple.DOCUMENT_CAT_CODE
tuple.DOCUMENT_CAT_DESC
tuple.DOCUMENT_TYPE_CODE
tuple.DOCUMENT_TYPE_DESC
tuple.DOCUMENT_SUBTYPE_CODE
tuple.DOCUMENT_SUBTYPE_DESC
tuple.DOCUMENT_DATE_FORMAT
jurisdiction
dba_name
doc_day
doc_month
doc_year
documentStateList
documentstatus
contentsize
createdBy
systemCreatedDateTime
systemModifiedDateTime
modifiedBy
documentVersion
initialVersionid
documentversionLabel
versionTags
documentStatusDateTime
documentTitle
firewall_privacy_booking
firewall_privacy_jurisdi
gci_no
gisClass
documentCreatedDateTime
LABEL
grmHold
LEGAL_NAME
lockOwner
lockCreationDateTime
merchant_id
pageCount
originationSystemBatchUid
originationSystemCaptureTime
originationSystemDocId
originationSystemName
prev_doc_id
prev_doc_source
recordCode
recordTriggerStartDateTime
rtm_id
sent_by
sourceDocDescription
updated_by


Read the BATCH TABLE
BATCHID
BATCH_START_DATE
BATCH_END_DATE

Query Oracle DB 

BATCH TABLE




from pyspark.sql import functions as F
from pyspark.sql.types import StringType

# Define a UDF to convert BSON UUID to string format
def bson_to_str_uuid(bson_uuid):
    return str(bson_uuid)

# Register the UDF
bson_to_str_uuid_udf = F.udf(bson_to_str_uuid, StringType())

# Source DataFrame: Assume Object_ID is already in string format
source_selected_df = source_df.select(
    F.col("Object_ID").alias("source_uuid"),
    F.col("DOMAIN"),
    F.col("DIVISION"),
    F.col("SUBDIVISION")
)

# Target DataFrame: Convert BSON UUID to string and select the relevant columns
target_selected_df = target_df.select(
    bson_to_str_uuid_udf(F.col("documentOS.documentOsGuid")).alias("target_uuid"),
    F.col("bankDocument.tenants.DOMAIN").alias("DOMAIN"),
    F.col("bankDocument.tenants.DIVISION").alias("DIVISION"),
    F.col("bankDocument.tenants.SUBDIVISION").alias("SUBDIVISION")
)

# Join DataFrames on UUID
comparison_df = source_selected_df.join(
    target_selected_df,
    on=F.col("source_uuid") == F.col("target_uuid"),
    how="inner"
)

# Check for mismatches in the selected columns
mismatch_conditions = [
    F.col(f"source.{col}") != F.col(f"target.{col}")
    for col in ["DOMAIN", "DIVISION", "SUBDIVISION"]
]

# Create a column to record mismatches
mismatch_df = comparison_df.withColumn(
    "mismatch_columns",
    F.expr("array(" + ", ".join([f"case when {cond} then '{col}' end" for cond, col in zip(mismatch_conditions, ["DOMAIN", "DIVISION", "SUBDIVISION"])]) + ")")
)

# Filter for mismatches
mismatch_df = mismatch_df.filter(F.size(F.col("mismatch_columns")) > 0)

# Reconciliation result
reconcile_check = "MATCHED" if mismatch_df.count() == 0 else "MISMATCHED"

# Log the result
logger.info(f"Data comparison result for BATCHID={batch['BATCHID']}: {reconcile_check}")

# Update the reconciliation status in the batch table
update_query = f"""
    UPDATE SparkReconBatchTable 
    SET RECONCILE_CHECK = '{reconcile_check}' 
    WHERE BATCHID = '{batch['BATCHID']}'
"""
cursor.execute(update_query)
conn.commit()

